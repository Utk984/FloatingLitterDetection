{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T13:50:17.741755Z","iopub.status.busy":"2024-11-18T13:50:17.741264Z","iopub.status.idle":"2024-11-18T13:50:50.955787Z","shell.execute_reply":"2024-11-18T13:50:50.954672Z","shell.execute_reply.started":"2024-11-18T13:50:17.741696Z"},"trusted":true},"outputs":[],"source":["# common imports\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from glob import glob\n","from numpy import zeros\n","from numpy.random import randint\n","import torch\n","import cv2\n","from statistics import mean\n","from torch.nn.functional import threshold, normalize\n","# Data Viz\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import cv2\n","\n","# Install dependencies\n","! pip install opencv-python pycocotools matplotlib onnxruntime onnx\n","! pip install git+https://github.com/facebookresearch/segment-anything.git\n","\n","\"\"\"Import Training data\"\"\"\n","# Adjust the data paths\n","# Training data paths\n","image_path = \"/kaggle/input/lufi-riversnap/LuFI-RiverSnap.v1/Train/Images\"\n","label_path = \"/kaggle/input/lufi-riversnap/LuFI-RiverSnap.v1/Train/Labels\"\n","\n","total_images = len(os.listdir(image_path))\n","all_image_paths = sorted(glob(image_path + \"/*.jpg\"))\n","print(f\"Total Number of Training Images: {total_images}\")\n","\n","total_labels = len(os.listdir(label_path))\n","all_label_paths = sorted(glob(label_path + \"/*.png\"))\n","print(f\"Total Number of Training Labels: {total_labels}\")\n","\n","train_image_paths = all_image_paths[0:total_images]\n","train_label_paths = all_label_paths[0:total_labels]\n","\n","\"\"\"Import Validation data\"\"\"\n","val_image_path = \"/kaggle/input/lufi-riversnap/LuFI-RiverSnap.v1/Val/Images\"\n","val_label_path = \"//kaggle/input/lufi-riversnap/LuFI-RiverSnap.v1/Val/labels\"\n","\n","val_total_images = len(os.listdir(val_image_path))\n","val_image_paths = sorted(glob(val_image_path + \"/*.jpg\"))\n","print(f\"Total Number of Validation Images: {val_total_images}\")\n","\n","val_total_labels = len(os.listdir(val_label_path))\n","val_label_paths = sorted(glob(val_label_path + \"/*.png\"))\n","print(f\"Total Number of Validation Labels: {val_total_labels}\")\n","\n","val_image_paths = val_image_paths[0:val_total_images]\n","val_label_paths = val_label_paths[0:val_total_labels]"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T13:50:58.205253Z","iopub.status.busy":"2024-11-18T13:50:58.204592Z","iopub.status.idle":"2024-11-18T13:51:15.914033Z","shell.execute_reply":"2024-11-18T13:51:15.913018Z","shell.execute_reply.started":"2024-11-18T13:50:58.205211Z"},"trusted":true},"outputs":[],"source":["\"\"\"Reading ground_truth_masks for training and validation\"\"\"\n","desired_size = (512, 512)\n","\n","# Read training masks\n","ground_truth_masks = {}\n","for k in range(len(train_label_paths)):\n","    gt_grayscale = cv2.imread(train_label_paths[k], cv2.IMREAD_GRAYSCALE)\n","    if desired_size is not None:\n","        gt_grayscale = cv2.resize(gt_grayscale, desired_size, interpolation=cv2.INTER_LINEAR)\n","    ground_truth_masks[k] = (gt_grayscale > 0)\n","\n","# Read validation masks\n","ground_truth_masksv = {}\n","for s in range(len(val_label_paths)):\n","    gt_grayscale = cv2.imread(val_label_paths[s], cv2.IMREAD_GRAYSCALE)\n","    if desired_size is not None:\n","        gt_grayscale = cv2.resize(gt_grayscale, desired_size, interpolation=cv2.INTER_LINEAR)\n","    ground_truth_masksv[s] = (gt_grayscale > 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-11-18T13:51:31.936093Z","iopub.status.busy":"2024-11-18T13:51:31.935245Z","iopub.status.idle":"2024-11-18T13:51:49.537490Z","shell.execute_reply":"2024-11-18T13:51:49.536546Z","shell.execute_reply.started":"2024-11-18T13:51:31.936052Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["\"\"\"Import SAM model\"\"\"\n","!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n","from segment_anything import SamPredictor, sam_model_registry\n","\n","# Specify model type and checkpoint path\n","model_type = 'vit_h'\n","checkpoint = 'sam_vit_h_4b8939.pth'  # Adjusted path\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","# Load the SAM model\n","sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n","sam_model.to(device)\n","sam_model.train()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T13:52:04.842801Z","iopub.status.busy":"2024-11-18T13:52:04.841939Z","iopub.status.idle":"2024-11-18T13:52:46.452038Z","shell.execute_reply":"2024-11-18T13:52:46.451055Z","shell.execute_reply.started":"2024-11-18T13:52:04.842753Z"},"trusted":true},"outputs":[],"source":["\"\"\"Preprocess the images for training\"\"\"\n","from collections import defaultdict\n","from segment_anything.utils.transforms import ResizeLongestSide\n","\n","transformed_data = defaultdict(dict)\n","for k in range(len(train_image_paths)):\n","    image = cv2.imread(train_image_paths[k])\n","    if desired_size is not None:\n","        image = cv2.resize(image, desired_size, interpolation=cv2.INTER_LINEAR)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n","    input_image = transform.apply_image(image)\n","    input_image_torch = torch.as_tensor(input_image, device=device)\n","    transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n","\n","    input_image = sam_model.preprocess(transformed_image)\n","    original_image_size = image.shape[:2]\n","    input_size = tuple(transformed_image.shape[-2:])\n","\n","    transformed_data[k]['image'] = input_image\n","    transformed_data[k]['input_size'] = input_size\n","    transformed_data[k]['original_image_size'] = original_image_size"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T13:53:32.790520Z","iopub.status.busy":"2024-11-18T13:53:32.790096Z","iopub.status.idle":"2024-11-18T13:53:32.797780Z","shell.execute_reply":"2024-11-18T13:53:32.796836Z","shell.execute_reply.started":"2024-11-18T13:53:32.790480Z"},"trusted":true},"outputs":[],"source":["\"\"\"Set up the optimizer and Loss\"\"\"\n","lr = 1e-5\n","wd = 0\n","optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n","loss_fn = torch.nn.BCEWithLogitsLoss()\n","keys = list(ground_truth_masks.keys())\n","keys1 = list(ground_truth_masksv.keys())\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","batch_size = 64\n","num_epochs = 2"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T13:53:38.590956Z","iopub.status.busy":"2024-11-18T13:53:38.589947Z","iopub.status.idle":"2024-11-18T13:53:38.602529Z","shell.execute_reply":"2024-11-18T13:53:38.601488Z","shell.execute_reply.started":"2024-11-18T13:53:38.590899Z"},"trusted":true},"outputs":[],"source":["\"\"\"Fine-tuning SAM using Training data\"\"\"\n","def calculate_accuracy(predictions, targets):\n","    binary_predictions = (predictions > 0.5).float()\n","    accuracy = (binary_predictions == targets).float().mean()\n","    return accuracy.item()\n","\n","def train_on_batch(keys, batch_start, batch_end):\n","    batch_losses = []\n","    batch_accuracies = []\n","\n","    for k in keys[batch_start:batch_end]:\n","        input_image = transformed_data[k]['image'].to(device)\n","        input_size = transformed_data[k]['input_size']\n","        original_image_size = transformed_data[k]['original_image_size']\n","\n","        with torch.no_grad():\n","            image_embedding = sam_model.image_encoder(input_image)\n","\n","            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","                points=None,\n","                boxes=None,\n","                masks=None,\n","            )\n","\n","        low_res_masks, iou_predictions = sam_model.mask_decoder(\n","            image_embeddings=image_embedding,\n","            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n","            sparse_prompt_embeddings=sparse_embeddings,\n","            dense_prompt_embeddings=dense_embeddings,\n","            multimask_output=False,\n","        )\n","\n","        upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n","        binary_mask = (threshold(torch.sigmoid(upscaled_masks), 0.5, 0))\n","        gt_mask_resized = torch.from_numpy(np.resize(ground_truth_masks[k], (1, 1, ground_truth_masks[k].shape[0], ground_truth_masks[k].shape[1]))).to(device)\n","        gt_mask_resized = gt_mask_resized > 0.5\n","        gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n","\n","        loss = loss_fn(binary_mask, gt_binary_mask)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        batch_losses.append(loss.item())\n","\n","        # Calculate accuracy for training data\n","        train_accuracy = calculate_accuracy(binary_mask, gt_binary_mask)\n","        batch_accuracies.append(train_accuracy)\n","\n","    return batch_losses, batch_accuracies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-18T13:53:43.729094Z","iopub.status.busy":"2024-11-18T13:53:43.728485Z"},"trusted":true},"outputs":[],"source":["# Training loop\n","losses = []\n","val_losses = []\n","accuracies = []\n","best_val_loss = float('inf')  # Initialize best validation loss to positive infinity\n","val_acc = []\n","\n","for epoch in range(num_epochs):\n","    epoch_losses = []\n","    epoch_accuracies = []\n","\n","    # Training loop with batch processing\n","    for batch_start in range(0, len(keys), batch_size):\n","        batch_end = min(batch_start + batch_size, len(keys))\n","\n","        batch_losses, batch_accuracies = train_on_batch(keys, batch_start, batch_end)\n","\n","        # Calculate accuracy for the current batch\n","        batch_accuracy = mean(batch_accuracies)\n","        epoch_accuracies.extend(batch_accuracies)\n","\n","        # Calculate mean training loss for the current batch\n","        batch_loss = mean(batch_losses)\n","        epoch_losses.append(batch_loss)\n","\n","        print(f'Batch: [{batch_start+1}-{batch_end}]')\n","        print(f'Batch Loss: {batch_loss}')\n","        print(f'Batch Accuracy: {batch_accuracy}')\n","\n","    # Calculate mean training loss for the current epoch\n","    mean_train_loss = mean(epoch_losses)\n","    mean_train_accuracy = mean(epoch_accuracies)\n","    losses.append(mean_train_loss)\n","    accuracies.append(mean_train_accuracy)\n","\n","    print(f'EPOCH: {epoch}')\n","    print(f'Mean training loss: {mean_train_loss}')\n","    print(f'Mean training accuracy: {mean_train_accuracy}')\n","\n","    predictor_tuned = SamPredictor(sam_model)\n","\n","    # Validation loop\n","    val_loss = 0.0\n","    val_accuracy = 0.0\n","    num_val_examples = 0\n","    with torch.no_grad():\n","        for s in keys1[:len(val_image_paths)]:\n","            image = cv2.imread(val_image_paths[s])\n","            if desired_size is not None:\n","                image = cv2.resize(image, desired_size, interpolation=cv2.INTER_LINEAR)\n","\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            # Forward pass on validation data\n","            predictor_tuned.set_image(image)\n","\n","            masks_tuned, _, _ = predictor_tuned.predict(\n","                point_coords=None,\n","                box=None,\n","                multimask_output=False,\n","            )\n","\n","            gt_mask_resized = torch.from_numpy(np.resize(ground_truth_masksv[s], (1, 1, ground_truth_masksv[s].shape[0], ground_truth_masksv[s].shape[1]))).to(device)\n","            gt_mask_resized = gt_mask_resized > 0.5\n","            gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n","            masks_tuned1 = torch.as_tensor(masks_tuned > 0, dtype=torch.float32)\n","            new_tensor = masks_tuned1.unsqueeze(0).to(device)\n","\n","            # Calculate validation loss\n","            val_loss += loss_fn(new_tensor, gt_binary_mask).item()\n","\n","            # Calculate accuracy for validation data\n","            val_accuracy += calculate_accuracy(new_tensor, gt_binary_mask)\n","            num_val_examples += 1\n","\n","    # Calculate mean validation loss for the current epoch\n","    val_loss /= num_val_examples\n","    val_losses.append(val_loss)\n","    print(f'Mean validation loss: {val_loss}')\n","\n","    # Calculate mean validation accuracy for the current epoch\n","    mean_val_accuracy = val_accuracy / num_val_examples\n","    val_acc.append(mean_val_accuracy)\n","    print(f'Mean validation accuracy: {mean_val_accuracy}')\n","\n","    # Save the model checkpoint if the validation accuracy improves\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        models_path = '/kaggle/working'  # Save in the working directory\n","        torch.save(sam_model.state_dict(), os.path.join(models_path, 'SAM5122weights_ViTB.pth'))\n","\n","    # Clear GPU cache after each epoch\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"Testing fine-tuned SAM model\"\"\"\n","# Test data paths\n","test_image_path = \"/kaggle/input/lufi-riversnap/LuFI-RiverSnap.v1/Test/Images\"\n","test_label_path = \"/kaggle/input/lufi-riversnap/LuFI-RiverSnap.v1/Test/Labels\"\n","\n","test_total_images = len(os.listdir(test_image_path))\n","test_image_paths = sorted(glob(test_image_path + \"/*.jpg\"))\n","print(f\"Total Number of Test Images: {test_total_images}\")\n","\n","test_total_labels = len(os.listdir(test_label_path))\n","test_label_paths = sorted(glob(test_label_path + \"/*.png\"))\n","print(f\"Total Number of Test Labels: {test_total_labels}\")\n","\n","test_image_paths = test_image_paths[0:test_total_images]\n","test_label_paths = test_label_paths[0:test_total_labels]"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-11-13T19:24:53.320456Z","iopub.status.busy":"2024-11-13T19:24:53.320042Z","iopub.status.idle":"2024-11-13T19:24:59.029445Z","shell.execute_reply":"2024-11-13T19:24:59.028344Z","shell.execute_reply.started":"2024-11-13T19:24:53.320415Z"},"trusted":true},"outputs":[],"source":["\"\"\"Ground truth masks for testing\"\"\"\n","ground_truth_test_masks = {}\n","for k in range(len(test_image_paths)):\n","    gt_grayscale = cv2.imread(test_label_paths[k], cv2.IMREAD_GRAYSCALE)\n","    ground_truth_test = (gt_grayscale > 0).astype(np.float32)\n","    if desired_size is not None:\n","        ground_truth_test = cv2.resize(ground_truth_test, desired_size, interpolation=cv2.INTER_NEAREST)\n","    ground_truth_test_masks[k] = ground_truth_test"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-11-13T19:25:20.999840Z","iopub.status.busy":"2024-11-13T19:25:20.998863Z","iopub.status.idle":"2024-11-13T19:29:49.582615Z","shell.execute_reply":"2024-11-13T19:29:49.581566Z","shell.execute_reply.started":"2024-11-13T19:25:20.999793Z"},"trusted":true},"outputs":[],"source":["\"\"\"Prediction using Fine-tuned model\"\"\"\n","masks_tuned_list = {}\n","images_tuned_list = {}\n","for k in range(len(test_image_paths)):\n","    # Load the image and convert color space\n","    image = cv2.cvtColor(cv2.imread(test_image_paths[k]), cv2.COLOR_BGR2RGB)\n","    if desired_size is not None:\n","        image = cv2.resize(image, desired_size, interpolation=cv2.INTER_LINEAR)\n","\n","    predictor_tuned.set_image(image)\n","\n","    # Perform prediction using predictor_tuned object\n","    masks_tuned, _, _ = predictor_tuned.predict(\n","        point_coords=None,\n","        box=None,\n","        multimask_output=False,\n","    )\n","\n","    # Get the first mask from the predictions\n","    kk = masks_tuned[0, :, :]\n","    binary_mask = (kk > 0).astype(np.float32)\n","    images_tuned_list[k] = image\n","    masks_tuned_list[k] = binary_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-13T19:39:01.315554Z","iopub.status.busy":"2024-11-13T19:39:01.314667Z","iopub.status.idle":"2024-11-13T19:39:03.653091Z","shell.execute_reply":"2024-11-13T19:39:03.652220Z","shell.execute_reply.started":"2024-11-13T19:39:01.315513Z"},"trusted":true},"outputs":[],"source":["\"\"\"Plot results on all of the Test data\"\"\"\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Example plotting code (adjust as needed)\n","for idx in range(5):\n","    plt.figure(figsize=(10, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(images_tuned_list[idx])\n","    plt.axis('off')\n","    plt.title('Original Image')\n","\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(images_tuned_list[idx])\n","    plt.imshow(masks_tuned_list[idx], alpha=0.5)\n","    plt.axis('off')\n","    plt.title('Predicted Mask Overlay')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Assuming the model is already trained and the predictor_tuned is available\n","# Add this code after your existing code\n","\n","import os\n","import cv2\n","import numpy as np\n","from glob import glob\n","import matplotlib.pyplot as plt\n","\n","# Provide the path to your new images\n","new_images_path = \"/kaggle/input/flow-img-sample/images\"  # Replace 'your-new-images' with your dataset name\n","\n","# Get all image paths\n","new_image_paths = sorted(glob(os.path.join(new_images_path, \"*.*\")))  # Adjust the pattern if needed\n","\n","# Create directories to save the predicted masks if they don't exist\n","output_masks_path = '/kaggle/working/flow-img/predicted_masks'\n","if not os.path.exists(output_masks_path):\n","    os.makedirs(output_masks_path)\n","\n","# Set desired_size if needed\n","# If you trained with desired_size = None, set this to None\n","desired_size = (512, 512)  # Or None if you used original image sizes during training\n","\n","# Loop over the new images and generate predictions\n","for idx, image_path in enumerate(new_image_paths):\n","    # Load and preprocess the image\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        print(f\"Warning: Unable to read image at {image_path}\")\n","        continue\n","    if desired_size is not None:\n","        image = cv2.resize(image, desired_size, interpolation=cv2.INTER_LINEAR)\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    # Set the image for the predictor\n","    predictor_tuned.set_image(image_rgb)\n","\n","    # Perform prediction\n","    masks, scores, logits = predictor_tuned.predict(\n","        point_coords=None,\n","        box=None,\n","        multimask_output=False,\n","    )\n","\n","    # Get the first mask\n","    mask = masks[0]\n","    binary_mask = (mask > 0).astype(np.uint8)\n","\n","    # Save the predicted mask to disk\n","    # We'll save the mask as a PNG image where the mask is white (255) and background is black (0)\n","    mask_filename = os.path.basename(image_path)\n","    mask_filename = os.path.splitext(mask_filename)[0] + '_mask.png'\n","    mask_save_path = os.path.join(output_masks_path, mask_filename)\n","    cv2.imwrite(mask_save_path, binary_mask * 255)  # Multiply by 255 to convert binary mask to 0-255 range\n","\n","    # Optionally display the image and mask\n","    plt.figure(figsize=(10, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(image_rgb)\n","    plt.axis('off')\n","    plt.title('Original Image')\n","\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(image_rgb)\n","    plt.imshow(binary_mask, alpha=0.5, cmap='jet')  # You can change the colormap if you like\n","    plt.axis('off')\n","    plt.title('Predicted Mask Overlay')\n","\n","    plt.show()\n","\n","    print(f\"Processed image {idx + 1}/{len(new_image_paths)}: {image_path}\")\n","    print(f\"Predicted mask saved to: {mask_save_path}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4716612,"sourceId":8008020,"sourceType":"datasetVersion"},{"datasetId":6080299,"sourceId":9898511,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
