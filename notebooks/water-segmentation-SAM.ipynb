{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-11-13T19:00:17.286677Z","iopub.status.busy":"2024-11-13T19:00:17.285933Z","iopub.status.idle":"2024-11-13T19:00:35.357528Z","shell.execute_reply":"2024-11-13T19:00:35.356397Z","shell.execute_reply.started":"2024-11-13T19:00:17.286639Z"},"trusted":true},"outputs":[],"source":["\"\"\"Reading ground_truth_masks for training and validation\"\"\"\n","desired_size = (512, 512)\n","\n","# Read training masks\n","ground_truth_masks = {}\n","for k in range(len(train_label_paths)):\n","    gt_grayscale = cv2.imread(train_label_paths[k], cv2.IMREAD_GRAYSCALE)\n","    if desired_size is not None:\n","        gt_grayscale = cv2.resize(gt_grayscale, desired_size, interpolation=cv2.INTER_LINEAR)\n","    ground_truth_masks[k] = (gt_grayscale > 0)\n","\n","# Read validation masks\n","ground_truth_masksv = {}\n","for s in range(len(val_label_paths)):\n","    gt_grayscale = cv2.imread(val_label_paths[s], cv2.IMREAD_GRAYSCALE)\n","    if desired_size is not None:\n","        gt_grayscale = cv2.resize(gt_grayscale, desired_size, interpolation=cv2.INTER_LINEAR)\n","    ground_truth_masksv[s] = (gt_grayscale > 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-13T19:04:08.553884Z","iopub.status.busy":"2024-11-13T19:04:08.552709Z","iopub.status.idle":"2024-11-13T19:05:07.849857Z","shell.execute_reply":"2024-11-13T19:05:07.848852Z","shell.execute_reply.started":"2024-11-13T19:04:08.553828Z"},"trusted":true},"outputs":[],"source":["\"\"\"Import SAM model\"\"\"\n","!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n","from segment_anything import SamPredictor, sam_model_registry\n","\n","# Specify model type and checkpoint path\n","model_type = 'vit_h'\n","checkpoint = 'sam_vit_h_4b8939.pth'  # Adjusted path\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","# Load the SAM model\n","sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n","sam_model.to(device)\n","sam_model.train()\n","\n","\"\"\"Preprocess the images for training\"\"\"\n","from collections import defaultdict\n","from segment_anything.utils.transforms import ResizeLongestSide\n","\n","transformed_data = defaultdict(dict)\n","for k in range(len(train_image_paths)):\n","    image = cv2.imread(train_image_paths[k])\n","    if desired_size is not None:\n","        image = cv2.resize(image, desired_size, interpolation=cv2.INTER_LINEAR)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n","    input_image = transform.apply_image(image)\n","    input_image_torch = torch.as_tensor(input_image, device=device)\n","    transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n","\n","    input_image = sam_model.preprocess(transformed_image)\n","    original_image_size = image.shape[:2]\n","    input_size = tuple(transformed_image.shape[-2:])\n","\n","    transformed_data[k]['image'] = input_image\n","    transformed_data[k]['input_size'] = input_size\n","    transformed_data[k]['original_image_size'] = original_image_size"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-11-13T19:05:53.127996Z","iopub.status.busy":"2024-11-13T19:05:53.127006Z","iopub.status.idle":"2024-11-13T19:05:53.135034Z","shell.execute_reply":"2024-11-13T19:05:53.134050Z","shell.execute_reply.started":"2024-11-13T19:05:53.127948Z"},"trusted":true},"outputs":[],"source":["\"\"\"Set up the optimizer and Loss\"\"\"\n","lr = 1e-5\n","wd = 0\n","optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n","loss_fn = torch.nn.BCEWithLogitsLoss()\n","keys = list(ground_truth_masks.keys())\n","keys1 = list(ground_truth_masksv.keys())\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","batch_size = 64\n","num_epochs = 1"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-11-13T19:06:10.148872Z","iopub.status.busy":"2024-11-13T19:06:10.148473Z","iopub.status.idle":"2024-11-13T19:06:10.160824Z","shell.execute_reply":"2024-11-13T19:06:10.159829Z","shell.execute_reply.started":"2024-11-13T19:06:10.148833Z"},"trusted":true},"outputs":[],"source":["\"\"\"Fine-tuning SAM using Training data\"\"\"\n","def calculate_accuracy(predictions, targets):\n","    binary_predictions = (predictions > 0.5).float()\n","    accuracy = (binary_predictions == targets).float().mean()\n","    return accuracy.item()\n","\n","def train_on_batch(keys, batch_start, batch_end):\n","    batch_losses = []\n","    batch_accuracies = []\n","\n","    for k in keys[batch_start:batch_end]:\n","        input_image = transformed_data[k]['image'].to(device)\n","        input_size = transformed_data[k]['input_size']\n","        original_image_size = transformed_data[k]['original_image_size']\n","\n","        with torch.no_grad():\n","            image_embedding = sam_model.image_encoder(input_image)\n","\n","            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","                points=None,\n","                boxes=None,\n","                masks=None,\n","            )\n","\n","        low_res_masks, iou_predictions = sam_model.mask_decoder(\n","            image_embeddings=image_embedding,\n","            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n","            sparse_prompt_embeddings=sparse_embeddings,\n","            dense_prompt_embeddings=dense_embeddings,\n","            multimask_output=False,\n","        )\n","\n","        upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n","        binary_mask = (threshold(torch.sigmoid(upscaled_masks), 0.5, 0))\n","        gt_mask_resized = torch.from_numpy(np.resize(ground_truth_masks[k], (1, 1, ground_truth_masks[k].shape[0], ground_truth_masks[k].shape[1]))).to(device)\n","        gt_mask_resized = gt_mask_resized > 0.5\n","        gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n","\n","        loss = loss_fn(binary_mask, gt_binary_mask)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        batch_losses.append(loss.item())\n","\n","        # Calculate accuracy for training data\n","        train_accuracy = calculate_accuracy(binary_mask, gt_binary_mask)\n","        batch_accuracies.append(train_accuracy)\n","\n","    return batch_losses, batch_accuracies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-13T19:24:45.312303Z","iopub.status.busy":"2024-11-13T19:24:45.311841Z","iopub.status.idle":"2024-11-13T19:24:45.435611Z","shell.execute_reply":"2024-11-13T19:24:45.434694Z","shell.execute_reply.started":"2024-11-13T19:24:45.312248Z"},"trusted":true},"outputs":[],"source":["\"\"\"Testing fine-tuned SAM model\"\"\"\n","# Test data paths\n","test_image_path = \"/kaggle/input/lufi-riversnap/LuFI-RiverSnap.v1/Test/Images\"\n","test_label_path = \"/kaggle/input/lufi-riversnap/LuFI-RiverSnap.v1/Test/Labels\"\n","\n","test_total_images = len(os.listdir(test_image_path))\n","test_image_paths = sorted(glob(test_image_path + \"/*.jpg\"))\n","print(f\"Total Number of Test Images: {test_total_images}\")\n","\n","test_total_labels = len(os.listdir(test_label_path))\n","test_label_paths = sorted(glob(test_label_path + \"/*.png\"))\n","print(f\"Total Number of Test Labels: {test_total_labels}\")\n","\n","test_image_paths = test_image_paths[0:test_total_images]\n","test_label_paths = test_label_paths[0:test_total_labels]"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-11-13T19:24:53.320456Z","iopub.status.busy":"2024-11-13T19:24:53.320042Z","iopub.status.idle":"2024-11-13T19:24:59.029445Z","shell.execute_reply":"2024-11-13T19:24:59.028344Z","shell.execute_reply.started":"2024-11-13T19:24:53.320415Z"},"trusted":true},"outputs":[],"source":["\"\"\"Ground truth masks for testing\"\"\"\n","ground_truth_test_masks = {}\n","for k in range(len(test_image_paths)):\n","    gt_grayscale = cv2.imread(test_label_paths[k], cv2.IMREAD_GRAYSCALE)\n","    ground_truth_test = (gt_grayscale > 0).astype(np.float32)\n","    if desired_size is not None:\n","        ground_truth_test = cv2.resize(ground_truth_test, desired_size, interpolation=cv2.INTER_NEAREST)\n","    ground_truth_test_masks[k] = ground_truth_test"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-11-13T19:25:20.999840Z","iopub.status.busy":"2024-11-13T19:25:20.998863Z","iopub.status.idle":"2024-11-13T19:29:49.582615Z","shell.execute_reply":"2024-11-13T19:29:49.581566Z","shell.execute_reply.started":"2024-11-13T19:25:20.999793Z"},"trusted":true},"outputs":[],"source":["\"\"\"Prediction using Fine-tuned model\"\"\"\n","masks_tuned_list = {}\n","images_tuned_list = {}\n","for k in range(len(test_image_paths)):\n","    # Load the image and convert color space\n","    image = cv2.cvtColor(cv2.imread(test_image_paths[k]), cv2.COLOR_BGR2RGB)\n","    if desired_size is not None:\n","        image = cv2.resize(image, desired_size, interpolation=cv2.INTER_LINEAR)\n","\n","    predictor_tuned.set_image(image)\n","\n","    # Perform prediction using predictor_tuned object\n","    masks_tuned, _, _ = predictor_tuned.predict(\n","        point_coords=None,\n","        box=None,\n","        multimask_output=False,\n","    )\n","\n","    # Get the first mask from the predictions\n","    kk = masks_tuned[0, :, :]\n","    binary_mask = (kk > 0).astype(np.float32)\n","    images_tuned_list[k] = image\n","    masks_tuned_list[k] = binary_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-13T19:41:23.241308Z","iopub.status.busy":"2024-11-13T19:41:23.240608Z","iopub.status.idle":"2024-11-13T19:42:40.820240Z","shell.execute_reply":"2024-11-13T19:42:40.819333Z","shell.execute_reply.started":"2024-11-13T19:41:23.241242Z"},"trusted":true},"outputs":[],"source":["# Assuming the model is already trained and the predictor_tuned is available\n","# Add this code after your existing code\n","\n","import os\n","import cv2\n","import numpy as np\n","from glob import glob\n","import matplotlib.pyplot as plt\n","\n","# Provide the path to your new images\n","new_images_path = \"/kaggle/input/flow-img-sample/images\"  # Replace 'your-new-images' with your dataset name\n","\n","# Get all image paths\n","new_image_paths = sorted(glob(os.path.join(new_images_path, \"*.*\")))  # Adjust the pattern if needed\n","\n","# Create directories to save the predicted masks if they don't exist\n","output_masks_path = '/kaggle/working/flow-img/predicted_masks'\n","if not os.path.exists(output_masks_path):\n","    os.makedirs(output_masks_path)\n","\n","# Set desired_size if needed\n","# If you trained with desired_size = None, set this to None\n","desired_size = (512, 512)  # Or None if you used original image sizes during training\n","\n","# Loop over the new images and generate predictions\n","for idx, image_path in enumerate(new_image_paths):\n","    # Load and preprocess the image\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        print(f\"Warning: Unable to read image at {image_path}\")\n","        continue\n","    if desired_size is not None:\n","        image = cv2.resize(image, desired_size, interpolation=cv2.INTER_LINEAR)\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    # Set the image for the predictor\n","    predictor_tuned.set_image(image_rgb)\n","\n","    # Perform prediction\n","    masks, scores, logits = predictor_tuned.predict(\n","        point_coords=None,\n","        box=None,\n","        multimask_output=False,\n","    )\n","\n","    # Get the first mask\n","    mask = masks[0]\n","    binary_mask = (mask > 0).astype(np.uint8)\n","\n","    # Save the predicted mask to disk\n","    # We'll save the mask as a PNG image where the mask is white (255) and background is black (0)\n","    mask_filename = os.path.basename(image_path)\n","    mask_filename = os.path.splitext(mask_filename)[0] + '_mask.png'\n","    mask_save_path = os.path.join(output_masks_path, mask_filename)\n","    cv2.imwrite(mask_save_path, binary_mask * 255)  # Multiply by 255 to convert binary mask to 0-255 range\n","\n","    # Optionally display the image and mask\n","    plt.figure(figsize=(10, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(image_rgb)\n","    plt.axis('off')\n","    plt.title('Original Image')\n","\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(image_rgb)\n","    plt.imshow(binary_mask, alpha=0.5, cmap='jet')  # You can change the colormap if you like\n","    plt.axis('off')\n","    plt.title('Predicted Mask Overlay')\n","\n","    plt.show()\n","\n","    print(f\"Processed image {idx + 1}/{len(new_image_paths)}: {image_path}\")\n","    print(f\"Predicted mask saved to: {mask_save_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"Compute evaluation metrics\"\"\"\n","def binary_segmentation_metrics(predictions, targets):\n","    # Convert predictions and targets to numpy arrays\n","    predictions = predictions.squeeze()\n","    targets = targets\n","    # Convert predictions to binary values (0 or 1)\n","    predictions_binary = (predictions > 0.5).astype(int)\n","    targets_binary = targets.astype(int)\n","\n","    # True Positives (TP): prediction and target both are positive\n","    TP = np.sum((predictions_binary == 1) & (targets == 1))\n","\n","    # False Positives (FP): prediction is positive but target is negative\n","    FP = np.sum((predictions_binary == 1) & (targets == 0))\n","\n","    # False Negatives (FN): prediction is negative but target is positive\n","    FN = np.sum((predictions_binary == 0) & (targets == 1))\n","\n","    # True Negatives (TN): prediction and target both are negative\n","    TN = np.sum((predictions_binary == 0) & (targets == 0))\n","\n","    eps = 1e-5\n","    accuracy = (TP + TN + eps) / (TP + FP + FN + TN + eps)\n","    precision = (TP + eps) / (TP + FP + eps)\n","    recall = (TP + eps) / (TP + FN + eps)\n","    f_score = 2 * (precision * recall) / (precision + recall + eps)\n","    dice = (2 * TP + eps) / (2 * TP + FP + FN + eps)\n","    iou = (TP + eps) / (TP + FP + FN + eps)\n","\n","    total = TP + FP + FN + TN\n","    p_o = (TP + TN) / total\n","    p_e = ((TP + FP) * (TP + FN) + (FN + TN) * (FP + TN)) / (total ** 2 + eps)\n","    kappa = (p_o - p_e) / (1 - p_e + eps)\n","\n","    return accuracy, precision, recall, f_score, iou, kappa, FP, FN, TP, TN, dice\n","\n","def calculate_average_metrics(predictions_list, targets_list):\n","    num_masks = len(predictions_list)\n","\n","    total_metrics = {\n","        'accuracy': 0.0,\n","        'precision': 0.0,\n","        'recall': 0.0,\n","        'f_score': 0.0,\n","        'iou': 0.0,\n","        'kappa': 0.0,\n","        'FP': 0,\n","        'FN': 0,\n","        'MAR': 0.0,\n","        'FAR': 0.0,\n","        'dice': 0.0\n","    }\n","\n","    for i in range(num_masks):\n","        metrics = binary_segmentation_metrics(predictions_list[i], targets_list[i])\n","\n","        for metric_name, value in zip(total_metrics.keys(), metrics):\n","            total_metrics[metric_name] += value\n","\n","        total_metrics['MAR'] += metrics[8] / (metrics[8] + metrics[9] + 1e-5)\n","        total_metrics['FAR'] += metrics[6] / (metrics[9] + metrics[6] + 1e-5)\n","\n","    avg_metrics = {k: v / num_masks for k, v in total_metrics.items()}\n","\n","    return avg_metrics\n","\n","# Compute average metrics\n","avg_metrics = calculate_average_metrics(list(masks_tuned_list.values()), list(ground_truth_test_masks.values()))\n","\n","for metric_name, value in avg_metrics.items():\n","    print(f\"Average {metric_name}: {value}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4716612,"sourceId":8008020,"sourceType":"datasetVersion"},{"datasetId":6080299,"sourceId":9898511,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
